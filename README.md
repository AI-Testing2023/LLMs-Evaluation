# Large-Scale-Models-Evaluation-Survey

## Journal

## Conference

*NeurIPS (Datasets and Benchmarks Track)*<br>
[[HomePage](https://dblp.uni-trier.de/db/conf/nips/neurips2023.html)]

## Business

**Patronus AI**<br>
*人工智能模型评估公司, 美国.*<br>
[[HomePage](https://www.patronus.ai/)]

## Group

**天津大学自然语言处理实验室(tjunlp-lab)**<br>
*熊德意*<br>
*天津大学*<br>
[[HomePage](https://tjunlp-lab.github.io/)]
[[HomePage](https://dyxiong.github.io/)]
[[Github](https://github.com/tjunlp-lab)]

## Leaderboard

**Open LLM Leaderboard**<br>
*Huggingface.*<br>
[[HomePage](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)]

**Chatbot Arena**<br>
**MT-Bench**<br>
**Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.**<br>
*L Zheng, WL Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E **ng, H Zhang, et al.*<br>
*LMSYS Org.*<br>
[[HomePage](https://chat.lmsys.org/)]
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf)]

**AlpacaEval**<br>
**AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback.**<br>
*Y Dubois, CX Li, R Taori, T Zhang, I Gulrajani, J Ba, C Guestrin, PS Liang, TB Hashimoto.*<br>
[[HomePage](https://tatsu-lab.github.io/alpaca_eval/)]
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf)]

**SuperClue (中文语言理解测评基准)**<br>
*认知智能全国重点实验室.*<br>
[[HomePage](https://www.cluebenchmarks.com/index.html)]

**OpenCompass (司南)**<br>
*上海人工智能实验室.*<br>
[[HomePage](https://opencompass.org.cn/home)]
[[Github](https://opencompass.org.cn/home)]

**FlagEval (天秤大模型评测平台)**<br>
*北京智源研究院.*<br>
[[HomePage](https://flageval.baai.ac.cn/#/home)]

**LLMEval**<br>
**LLMEval: A Preliminary Study on How to Evaluate Large Language Models.**<br>
*Y Zhang, M Zhang, H Yuan, S Liu, Y Shi, T Gui, Q Zhang, X Huang.*<br>
*Fudan University.*<br>
Proceedings of the AAAI Conference on Artificial Intelligence, 2024.
[[HomePage](http://llmeval.com/index)]
[[Paper](https://ojs.aaai.org/index.php/AAAI/article/download/29934/31632)]
[[Github](https://github.com/llmeval/)]

## Review

**A Survey on Evaluation of Large Language Models.**<br>
*Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, W Ye, et al.*<br>
ACM Transactions on Intelligent Systems and Technology, 2023.
[[Paper](https://dl.acm.org/doi/pdf/10.1145/3641289)]
[[ArXiv](https://arxiv.org/pdf/2307.03109.pdf)]
[[Github](https://github.com/MLGroupJLU/LLM-eval-survey)]

## Large Language Models (LLMs)

### 1 Testing Capability

#### 1.1 General-Oriented Testing (GOT)

##### 1.1.1 Understanding

**Measuring massive multitask language understanding.**<br>
*D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt.*<br>
ICLR, 2021.
[[ArXiv](https://arxiv.org/pdf/2009.03300.pdf?trk=public_post_comment-text)]
[[Github](https://github.com/hendrycks/test)]
[[Datasets](https://huggingface.co/datasets/tasksource/mmlu)]

#### 1.2 Industry-Oriented Testing (IOT)

##### 1.2.1 Finance

**PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance.**<br>
*Q Xie, W Han, X Zhang, Y Lai, M Peng, A Lopez-Lira, J Huang.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2306.05443.pdf)]
[[Github](https://github.com/The-FinAI/PIXIU)]
[[Datasets](https://huggingface.co/ChanceFocus)]

#### 1.3 Application-Oriented Testing (AOT)

##### 1.3.1 AI Assistants

**GAIA: a benchmark for General AI Assistants.**<br>
*G Mialon, C Fourrier, C Swift, T Wolf, Y LeCun, T Scialom.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2311.12983.pdf?trk=public_post_comment-text)]
[[Datasets](https://huggingface.co/datasets/gaia-benchmark/GAIA)]

#### 1.4  Security-Oriented Testing (SOT)

##### 1.4.1 Content Security

**JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models.**<br>
*M Zhang, X Pan, M Yang.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2311.00286.pdf)]
[[Github](https://github.com/whitzard-ai/jade-db)]

### 2 Testing Datasets

#### 2.1 Data Generation

**DyVal: Graph-informed Dynamic Evaluation of Large Language Models.**<br>
*K Zhu, J Chen, J Wang, NZ Gong, D Yang, X Xie.*<br>
ICLR, 2024.

### 3 Testing Methods

#### 3.1 Result Evaluation

**JudgeLM: Fine-tuned Large Language Models are Scalable Judges.**<br>
*L Zhu, X Wang, X Wang.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2310.17631.pdf)]
[[Github](https://github.com/baaivision/JudgeLM)]

**CRITIQUELLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation.**<br>
*P Ke, B Wen, Z Feng, X Liu, X Lei, J Cheng, S Wang, A Zeng, Y Dong, H Wang, J Tang, and et al.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2311.18702.pdf)]
[[Github](https://github.com/thu-coai/CritiqueLLM)]

### 4 Testing Tools

**OpenCompass (司南)**<br>
*上海人工智能实验室, 2023.*<br>
[[HomePage](https://opencompass.org.cn/home)]

**OpenEval (开放式大模型综合评估)**<br>
*天津大学, 2023.*<br>
[[HomePage](http://openeval.org.cn/)]
[[Github](https://opencompass.org.cn/home)]

**LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking.**<br>
*F Dalvi, M Hasanain, S Boughorbel, B Mousi, S Abdaljalil, N Nazar, A Abdelali, and et al.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2308.04945.pdf)]
[[Github](https://github.com/qcri/LLMeBench/)]

## Multi-modal Models (MMMs)
