# Large-Scale-Models-Evaluation-Survey

## Journal

## Conference

*NeurIPS (Datasets and Benchmarks Track)*<br>
[[HomePage](https://dblp.uni-trier.de/db/conf/nips/neurips2023.html)]

## Business

**Patronus AI**<br>
*人工智能模型评估公司, 美国.*<br>
[[HomePage](https://www.patronus.ai/)]

## Group

**天津大学自然语言处理实验室(tjunlp-lab)**<br>
*熊德意*<br>
*天津大学*<br>
[[HomePage](https://tjunlp-lab.github.io/)]
[[HomePage](https://dyxiong.github.io/)]
[[Github](https://github.com/tjunlp-lab)]

## Leaderboard

**Open LLM Leaderboard**<br>
*Huggingface.*<br>
[[HomePage](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)]

**SuperClue (中文语言理解测评基准)**<br>
*认知智能全国重点实验室.*<br>
[[HomePage](https://www.cluebenchmarks.com/index.html)]

**OpenCompass (司南)**<br>
*上海人工智能实验室.*<br>
[[HomePage](https://opencompass.org.cn/home)]
[[Github](https://opencompass.org.cn/home)]

**FlagEval (天秤大模型评测平台)**<br>
*北京智源研究院.*<br>
[[HomePage](https://flageval.baai.ac.cn/#/home)]

## Review

**A Survey on Evaluation of Large Language Models.**<br>
*Y. Chang, X. Wang, et al.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2307.03109.pdf)]
[[Github](https://github.com/MLGroupJLU/LLM-eval-survey)]

## Large Language Models (LLMs)

### Testing Capability

#### General-Oriented Testing (GOT)

##### Understanding

**Measuring massive multitask language understanding.**<br>
*D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt.*<br>
ICLR, 2021.
[[HomePage](https://arxiv.org/pdf/2009.03300.pdf?trk=public_post_comment-text)]
[[Github](https://github.com/hendrycks/test)]
[[Datasets](https://huggingface.co/datasets/tasksource/mmlu)]

#### Industry-Oriented Testing (IOT)

##### Finance

**PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance.**<br>
*Q Xie, W Han, X Zhang, Y Lai, M Peng, A Lopez-Lira, J Huang.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2306.05443.pdf)]
[[Github](https://github.com/The-FinAI/PIXIU)]
[[Datasets](https://huggingface.co/ChanceFocus)]

#### Application-Oriented Testing (AOT)

##### AI Assistants

**GAIA: a benchmark for General AI Assistants.**<br>
*G Mialon, C Fourrier, C Swift, T Wolf, Y LeCun, T Scialom.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2311.12983.pdf?trk=public_post_comment-text)]
[[Datasets](https://huggingface.co/datasets/gaia-benchmark/GAIA)]

#### Security-Oriented Testing (SOT)

##### Content Security

**JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models.**<br>
*M Zhang, X Pan, M Yang.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2311.00286.pdf)]
[[Github](https://github.com/whitzard-ai/jade-db)]

### Testing Datasets

#### Data Generation

**DyVal: Graph-informed Dynamic Evaluation of Large Language Models.**<br>
*K Zhu, J Chen, J Wang, NZ Gong, D Yang, X Xie.*<br>
ICLR, 2024.

### Testing Methods

#### Result Evaluation

**JudgeLM: Fine-tuned Large Language Models are Scalable Judges.**<br>
*L Zhu, X Wang, X Wang.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2310.17631.pdf)]
[[Github](https://github.com/baaivision/JudgeLM)]

**CRITIQUELLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation.**<br>
*P Ke, B Wen, Z Feng, X Liu, X Lei, J Cheng, S Wang, A Zeng, Y Dong, H Wang, J Tang, and et al.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2311.18702.pdf)]
[[Github](https://github.com/thu-coai/CritiqueLLM)]

### Testing Tools

**OpenCompass (司南)**<br>
*上海人工智能实验室, 2023.*<br>
[[HomePage](https://opencompass.org.cn/home)]

**OpenEval (开放式大模型综合评估)**<br>
*天津大学, 2023.*<br>
[[HomePage](http://openeval.org.cn/)]
[[Github](https://opencompass.org.cn/home)]

**LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking.**<br>
*F Dalvi, M Hasanain, S Boughorbel, B Mousi, S Abdaljalil, N Nazar, A Abdelali, and et al.*<br>
ArXiv, 2023.
[[Github](https://github.com/qcri/LLMeBench/)]
