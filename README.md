# Large-Scale-Models-Evaluation-Survey

## Journal

## Conference

*NeurIPS (Datasets and Benchmarks Track)*<br>
[[HomePage](https://dblp.uni-trier.de/db/conf/nips/neurips2023.html)]

## Business

**Patronus AI**<br>
*人工智能模型评估公司, 美国.*<br>
[[HomePage](https://www.patronus.ai/)]

## Group

**天津大学自然语言处理实验室(tjunlp-lab)**<br>
*熊德意*<br>
*天津大学*<br>
[[HomePage](https://tjunlp-lab.github.io/)]
[[HomePage](https://dyxiong.github.io/)]
[[Github](https://github.com/tjunlp-lab)]

## Leaderboard

**Open LLM Leaderboard**<br>
*Huggingface.*<br>
[[HomePage](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)]

**SuperClue (中文语言理解测评基准)**<br>
*认知智能全国重点实验室.*<br>
[[HomePage](https://www.cluebenchmarks.com/index.html)]

**OpenCompass (司南)**<br>
*上海人工智能实验室.*<br>
[[HomePage](https://opencompass.org.cn/home)]
[[Github](https://opencompass.org.cn/home)]

**FlagEval (天秤大模型评测平台)**<br>
*北京智源研究院.*<br>
[[HomePage](https://flageval.baai.ac.cn/#/home)]

## Review

**A Survey on Evaluation of Large Language Models.**<br>
*Y. Chang, X. Wang, et al.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2307.03109.pdf)]
[[Github](https://github.com/MLGroupJLU/LLM-eval-survey)]

## Large Language Models (LLMs)

### 1 Testing Capability

#### 1.1 General-Oriented Testing (GOT)

##### 1.1.1 Understanding

**Measuring massive multitask language understanding.**<br>
*D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt.*<br>
ICLR, 2021.
[[HomePage](https://arxiv.org/pdf/2009.03300.pdf?trk=public_post_comment-text)]
[[Github](https://github.com/hendrycks/test)]
[[Datasets](https://huggingface.co/datasets/tasksource/mmlu)]

#### 1.2 Industry-Oriented Testing (IOT)

##### 1.2.1 Finance

**PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance.**<br>
*Q Xie, W Han, X Zhang, Y Lai, M Peng, A Lopez-Lira, J Huang.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2306.05443.pdf)]
[[Github](https://github.com/The-FinAI/PIXIU)]
[[Datasets](https://huggingface.co/ChanceFocus)]

#### 1.3 Application-Oriented Testing (AOT)

##### 1.3.1 AI Assistants

**GAIA: a benchmark for General AI Assistants.**<br>
*G Mialon, C Fourrier, C Swift, T Wolf, Y LeCun, T Scialom.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2311.12983.pdf?trk=public_post_comment-text)]
[[Datasets](https://huggingface.co/datasets/gaia-benchmark/GAIA)]

#### 1.4  Security-Oriented Testing (SOT)

##### 1.4.1 Content Security

**JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models.**<br>
*M Zhang, X Pan, M Yang.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2311.00286.pdf)]
[[Github](https://github.com/whitzard-ai/jade-db)]

### 2 Testing Datasets

#### 2.1 Data Generation

**DyVal: Graph-informed Dynamic Evaluation of Large Language Models.**<br>
*K Zhu, J Chen, J Wang, NZ Gong, D Yang, X Xie.*<br>
ICLR, 2024.

### 3 Testing Methods

#### 3.1 Result Evaluation

**JudgeLM: Fine-tuned Large Language Models are Scalable Judges.**<br>
*L Zhu, X Wang, X Wang.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2310.17631.pdf)]
[[Github](https://github.com/baaivision/JudgeLM)]

**CRITIQUELLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation.**<br>
*P Ke, B Wen, Z Feng, X Liu, X Lei, J Cheng, S Wang, A Zeng, Y Dong, H Wang, J Tang, and et al.*<br>
ArXiv, 2023.
[[HomePage](https://arxiv.org/pdf/2311.18702.pdf)]
[[Github](https://github.com/thu-coai/CritiqueLLM)]

### 4 Testing Tools

**OpenCompass (司南)**<br>
*上海人工智能实验室, 2023.*<br>
[[HomePage](https://opencompass.org.cn/home)]

**OpenEval (开放式大模型综合评估)**<br>
*天津大学, 2023.*<br>
[[HomePage](http://openeval.org.cn/)]
[[Github](https://opencompass.org.cn/home)]

**LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking.**<br>
*F Dalvi, M Hasanain, S Boughorbel, B Mousi, S Abdaljalil, N Nazar, A Abdelali, and et al.*<br>
ArXiv, 2023.
[[Github](https://github.com/qcri/LLMeBench/)]
